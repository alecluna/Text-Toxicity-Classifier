{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import tree, metrics    # Decision Tree\n",
    "from sklearn.linear_model import LogisticRegression # LogisticRegression\n",
    "from sklearn.svm import SVC # Support Vector Machine \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#reading in only 500k records\n",
    "toxic_comments_df = pd.read_csv('train.csv',nrows=100000)\n",
    "toxic_comments_df =  toxic_comments_df[['id','target','comment_text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  truth\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...    0.0\n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...    0.0\n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...    0.0\n",
       "3  59855  0.000000  Is this something I'll be able to install on m...    0.0\n",
       "4  59856  0.893617               haha you guys are a bunch of losers.    1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating our truth column. comment is toxic if target >= .5\n",
    "toxic_comments_df['truth'] = toxic_comments_df.target.apply(lambda x: 1 if x>=.5 else 0 ).astype('float32')\n",
    "toxic_comments_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>truth</th>\n",
       "      <th>InputData</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>this is so cool its like would you want your m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thank you this would make my life a lot less a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>this is such an urgent design problem kudos to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>is this something ill be able to install on my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>1.0</td>\n",
       "      <td>haha you guys are a bunch of losers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  truth  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...    0.0   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...    0.0   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...    0.0   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...    0.0   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.    1.0   \n",
       "\n",
       "                                           InputData  \n",
       "0  this is so cool its like would you want your m...  \n",
       "1  thank you this would make my life a lot less a...  \n",
       "2  this is such an urgent design problem kudos to...  \n",
       "3  is this something ill be able to install on my...  \n",
       "4                haha you guys are a bunch of losers  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments_df['InputData'] = toxic_comments_df['comment_text'].str.replace('[{}]'.format(string.punctuation), '')\n",
    "toxic_comments_df['InputData'] = toxic_comments_df['InputData'].apply(lambda x: \" \".join([ word.lower() if word[0].isalpha() else \"\"  for word in str(x).split()]))\n",
    "\n",
    "toxic_comments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of features 3929\n"
     ]
    }
   ],
   "source": [
    "vectorizerInput = sk_text.CountVectorizer(#max_features = 10000,\n",
    "                             min_df=.001, \n",
    "                             #max_df=.75,\n",
    "                            stop_words=['id','truth']\n",
    "                            )\n",
    "matrix = vectorizerInput.fit_transform(toxic_comments_df.InputData.values)\n",
    "print(\"# of features\", len(vectorizerInput.get_feature_names()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_text = pd.DataFrame(matrix.todense(), index=toxic_comments_df.id, columns=vectorizerInput.get_feature_names()).astype('float32')\n",
    "\n",
    "dataFrameWithHashTagHandlesAndTruths = pd.merge(df_text,  toxic_comments_df[['id','truth']].copy(), on='id').astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 3929)\n",
      "(80000,)\n",
      "(20000, 3929)\n",
      "(20000,)\n",
      "Wall time: 1.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = dataFrameWithHashTagHandlesAndTruths.drop(['id','truth'], axis=1)\n",
    "y = dataFrameWithHashTagHandlesAndTruths.truth.values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\avnee\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "E:\\Users\\avnee\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Scores:\n",
      "[0.9328402646445451, 0.9328402646445451, 0.9324825891633807, 0.9324519729932954]\n",
      "\n",
      "Best Models:\n",
      "['newton-cg', 'liblinear', 'sag', 'saga']\n",
      "\n",
      "Overall Best Score:\n",
      "newton-cg\n",
      "0.9310666074332825\n",
      "0.94175\n",
      "0.9328402646445451\n",
      "[[18407   256]\n",
      " [  909   428]]\n",
      "Wall time: 13min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "solvers = ['newton-cg', 'liblinear', 'sag', 'saga']\n",
    "best_scores = []\n",
    "best_models = []\n",
    "for current_solver in solvers:\n",
    "        logreg = LogisticRegression(solver=current_solver)\n",
    "        logreg.fit(X_train, y_train)\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        best_scores.append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "        best_models.append(current_solver)\n",
    "        \n",
    "best_solver = solvers[best_scores.index(max(best_scores))]\n",
    "\n",
    "\n",
    "print('Best Scores:')    \n",
    "print(best_scores)\n",
    "print()\n",
    "print('Best Models:')\n",
    "print(best_models)\n",
    "print()\n",
    "print('Overall Best Score:')\n",
    "print(best_solver)\n",
    "\n",
    "logreg = LogisticRegression(solver=best_solver)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\avnee\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "potenial_c = [.1,.2,.3,.4,.5,.6,.7,.8,.9,1.0]\n",
    "best_scores = []\n",
    "best_C = []\n",
    "for c in potenial_c:\n",
    "    clf = SVC(C=c, gamma='auto') \n",
    "    clf.fit(X_train, y_train) \n",
    "    y_pred = clf.predict(X_test)\n",
    "    best_scores.append(metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "    best_C.append(c)\n",
    "\n",
    "    \n",
    "best_c_val = potenial_c[best_scores.index(max(best_scores))]\n",
    "\n",
    "print('Best Scores:')    \n",
    "print(best_scores)\n",
    "print('Best C:')\n",
    "print(best_C)\n",
    "print('Overall Best Score:')\n",
    "print(best_c_val)git\n",
    "\n",
    "clf = SVC(C=best_c_val, gamma='auto') \n",
    "clf.fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_test)\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9185051448310593\n",
      "0.92075\n",
      "0.919597999088646\n",
      "[[17911   752]\n",
      " [  833   504]]\n",
      "Wall time: 5min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\avnee\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "E:\\Users\\avnee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n",
      "E:\\Users\\avnee\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DataConversionWarning: Data with input dtype float32 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_nn = scaler.transform(X_train)\n",
    "X_test_nn = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9332687807003315\n",
      "0.94285\n",
      "0.9352216392593429\n",
      "[[18386   277]\n",
      " [  866   471]]\n",
      "Wall time: 35min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000), solver = 'adam')\n",
    "mlp.fit(X_train_nn,y_train)\n",
    "y_pred = mlp.predict(X_test_nn)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
