{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding, Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target(value):\n",
    "    if value>=0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    '''\n",
    "    Credit goes to https://www.kaggle.com/gpreda/jigsaw-fast-compact-solution\n",
    "    '''\n",
    "    punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "    def clean_special_chars(text, punct):\n",
    "        for p in punct:\n",
    "            text = text.replace(p, ' ')\n",
    "        return text\n",
    "\n",
    "    data = data.astype(str).apply(lambda x: clean_special_chars(x, punct))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert a list to a tuple\n",
    "def totuple(a):\n",
    "    try:\n",
    "        return tuple(totuple(i) for i in a)\n",
    "    except TypeError:\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(cleaned_data, data_size):\n",
    "    if(cleaned_data == \"1\"): # get cleaned and preprosessed data\n",
    "        return pd.read_csv('data/train_preprocessed.csv',nrows=data_size) \n",
    "    else:\n",
    "        return pd.read_csv('data/train.csv',nrows=data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parameters(dct):\n",
    "    print(\"Parameters:\")\n",
    "    for item, amount in dct.items():\n",
    "        print(\"{} ({})\".format(item, amount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(train_df, max_words, max_len):  \n",
    "    train_df =  train_df[['id','target','comment_text']]\n",
    "    train_df['truth'] = train_df['target'].apply(target)\n",
    "    train_df['comment_text'] = preprocess(train_df['comment_text'])\n",
    "    \n",
    "    X = train_df.comment_text\n",
    "    Y = train_df.truth\n",
    "    le = LabelEncoder()\n",
    "    Y = le.fit_transform(Y)\n",
    "    X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)\n",
    "    \n",
    "    tok = Tokenizer(num_words=max_words)\n",
    "    tok.fit_on_texts(X_train)\n",
    "    sequences = tok.texts_to_sequences(X_train)\n",
    "    sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "    \n",
    "    return (sequences_matrix, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  BATCH_SIZE, LSTM_UNITS, MAX_LEN, MAX_WORDS, DROPOUT, BIDIRECTIONAL, DOUBLE_LSTM_LAYER\n",
    "\n",
    "def build_rnn(max_len , max_words, lstm_units, dropout, bidirectional, double_ltsm):\n",
    "#     calculate Params\n",
    "    DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "\n",
    "#     build RNN\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    \n",
    "    if(double_ltsm):\n",
    "        if(bidirectional):   \n",
    "            layer = Bidirectional(LSTM(lstm_units, return_sequences=True))(layer)\n",
    "        else:\n",
    "            layer = LSTM(lstm_units, return_sequences=True)(layer)\n",
    "        \n",
    "    if(bidirectional):   \n",
    "        layer = Bidirectional(LSTM(lstm_units))(layer)\n",
    "    else:\n",
    "        layer = LSTM(lstm_units)(layer)\n",
    "        \n",
    "    \n",
    "    layer = Dense(DENSE_HIDDEN_UNITS, name='Dense')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    \n",
    "    if(dropout):\n",
    "        layer = Dropout(0.3)(layer)\n",
    "    \n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  grid search - every combination will be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameterrs:\n",
      "CLEANED_DATA ([True, False])\n",
      "DATA_ROWS (50000)\n",
      "BATCH_SIZE ([512])\n",
      "LSTM_UNITS ([128])\n",
      "EPOCHS ([4])\n",
      "MAX_LEN ([150])\n",
      "MAX_WORDS ([2000])\n",
      "DROPOUT ([True])\n",
      "BIDIRECTIONAL ([True, False])\n",
      "DOUBLE_LSTM_LAYER ([True, False])\n"
     ]
    }
   ],
   "source": [
    "# feature sellection variables\n",
    "CLEANED_DATA = [True,\n",
    "                False\n",
    "               ]\n",
    "DATA_ROWS = 50000\n",
    "\n",
    "# Parameter tuning variablesmax_words = [2000] #\n",
    "BATCH_SIZE = [512] # How many comments are used for each training cycle \n",
    "LSTM_UNITS = [\n",
    "#     64, \n",
    "    128\n",
    "    ] # how many units are in the LSTM\n",
    "\n",
    "EPOCHS = [4] # num of times to go over entire training data set\n",
    "MAX_LEN = [150] # numder of words allowed before the comment is cut off\n",
    "MAX_WORDS = [2000] # number of features (if word isn't found, it's skipped)\n",
    "DROPOUT = [\n",
    "    True, \n",
    "#     False\n",
    "] # dropout layer ratio 0 for so drop out\n",
    "\n",
    "BIDIRECTIONAL = [\n",
    "    True, \n",
    "    False\n",
    "] # Whether or not to use the bidirectional STML layers\n",
    "DOUBLE_LSTM_LAYER = [\n",
    "    True, \n",
    "    False\n",
    "] # Whether or not to use the bidirectional STML layers\n",
    "\n",
    "help_display = dict(zip(\n",
    "    ['CLEANED_DATA','DATA_ROWS','BATCH_SIZE', 'LSTM_UNITS','EPOCHS', 'MAX_LEN', 'MAX_WORDS','DROPOUT', 'BIDIRECTIONAL', 'DOUBLE_LSTM_LAYER'],\n",
    "    [CLEANED_DATA,DATA_ROWS,BATCH_SIZE, LSTM_UNITS,EPOCHS, MAX_LEN, MAX_WORDS,DROPOUT, BIDIRECTIONAL, DOUBLE_LSTM_LAYER]\n",
    "))\n",
    "\n",
    "\n",
    "parameter_grid = np.array(np.meshgrid(CLEANED_DATA,\n",
    "                                      DATA_ROWS,\n",
    "                                      BATCH_SIZE, \n",
    "                                      LSTM_UNITS,\n",
    "                                      EPOCHS, \n",
    "                                      MAX_LEN, \n",
    "                                      MAX_WORDS,\n",
    "                                      DROPOUT, \n",
    "                                      BIDIRECTIONAL, \n",
    "                                      DOUBLE_LSTM_LAYER\n",
    "                                     )).T.reshape(-1,10)\n",
    "\n",
    "# print('bat, unit, epoc, len, words, drop, bid, 2-layers')\n",
    "print_parameters(help_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameterrs:\n",
      "CLEANED_DATA (1)\n",
      "DATA_ROWS (50000)\n",
      "BATCH_SIZE (512)\n",
      "LSTM_UNITS (128)\n",
      "EPOCHS (4)\n",
      "MAX_LEN (150)\n",
      "MAX_WORDS (2000)\n",
      "DROPOUT (1)\n",
      "BIDIRECTIONAL (1)\n",
      "DOUBLE_LSTM_LAYER (1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanj\\Anaconda3\\envs\\python3.6\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ryanj\\Anaconda3\\envs\\python3.6\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameterrs:\n",
      "CLEANED_DATA (1)\n",
      "DATA_ROWS (50000)\n",
      "BATCH_SIZE (512)\n",
      "LSTM_UNITS (128)\n",
      "EPOCHS (4)\n",
      "MAX_LEN (150)\n",
      "MAX_WORDS (2000)\n",
      "DROPOUT (1)\n",
      "BIDIRECTIONAL (1)\n",
      "DOUBLE_LSTM_LAYER (1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 150, 50)           100000    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 150, 256)          183296    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "Dense (Dense)                (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 809,633\n",
      "Trainable params: 809,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 32000 samples, validate on 8000 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "for use_cleaned_data in CLEANED_DATA:\n",
    "    train_df = getData(use_cleaned_data, DATA_ROWS)\n",
    "\n",
    "    for row in parameter_grid:\n",
    "\n",
    "        (CLEANED_DATA,DATA_ROWS,BATCH_SIZE, \n",
    "        LSTM_UNITS,EPOCHS, MAX_LEN, MAX_WORDS,\n",
    "        DROPOUT, BIDIRECTIONAL, DOUBLE_LSTM_LAYER) = totuple(row)\n",
    "\n",
    "        #Print display for tracking parameters\n",
    "        help_display = dict(zip(\n",
    "            ['CLEANED_DATA','DATA_ROWS','BATCH_SIZE', 'LSTM_UNITS','EPOCHS', 'MAX_LEN', 'MAX_WORDS','DROPOUT', 'BIDIRECTIONAL', 'DOUBLE_LSTM_LAYER'],\n",
    "            [CLEANED_DATA,DATA_ROWS,BATCH_SIZE, LSTM_UNITS,EPOCHS, MAX_LEN, MAX_WORDS,DROPOUT, BIDIRECTIONAL, DOUBLE_LSTM_LAYER]\n",
    "        ))\n",
    "        print_parameters(help_display)\n",
    "        sequences_matrix, Y_train = prepareData(train_df, MAX_WORDS, MAX_LEN) # get sequence matrix\n",
    "    #     max_len , max_words, lstm_units, dropout, bidirectional, double_ltsm\n",
    "        model = build_rnn(MAX_LEN, MAX_WORDS, LSTM_UNITS, DROPOUT, BIDIRECTIONAL, DOUBLE_LSTM_LAYER)\n",
    "\n",
    "        print_parameters(help_display)\n",
    "        print (model.summary())\n",
    "        model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "\n",
    "        model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "                  validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])\n",
    "    model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "              validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
