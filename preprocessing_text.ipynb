{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file seeks to preprocess the text for our Toxicity Classification kernal\n",
    "#### Adapted from: https://www.kaggle.com/fizzbuzz/toxic-data-preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import re\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Road Map\n",
    "# 1 make all words lowercase\n",
    "# 2 trim repeating words\n",
    "# 3\n",
    "# 4 trim repeating letters (where there are more than 2 repeating letters)\n",
    "# 5 use patterns to correct spelling of common words of interest \n",
    "# 6 remove/substitute non-typical characters \n",
    "# 7 remove unnessesary white space\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTokenizer(object):\n",
    "    def process_text(self, text):\n",
    "        raise NotImplemented\n",
    "\n",
    "    def process(self, texts):\n",
    "        for text in texts:\n",
    "            yield self.process_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    \n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)(u|[^a-z0-9 ])(c|[^a-z0-9 ])(k|[^a-z0-9 ])([^ ])*',\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k', 'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)', r'f\\*',\n",
    "            'feck ', ' fux ', 'f\\*\\*', \n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck', 'f ck'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
    "                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
    "        ],\n",
    "\n",
    "    ' ass hole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' transgender':\n",
    "        [\n",
    "            'transgender'\n",
    "        ],\n",
    "\n",
    "    ' gay ':\n",
    "        [\n",
    "            'gay', 'homo'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'd i c k'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "            'cunt', 'c u n t'\n",
    "        ],\n",
    "\n",
    "    ' bull shit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' jerk ':\n",
    "        [\n",
    "            'jerk'\n",
    "        ],\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots' 'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', '(s)([^a-z ]*)(h)([^a-z ]*)(i)([^a-z ]*)(t)', 'shite', '\\$hit', 's h i t', 'sh\\*tty',\n",
    "            'sh\\*ty'\n",
    "        ],\n",
    "\n",
    "    ' shit hole ':\n",
    "        [\n",
    "            'shythole', 'sh\\*thole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' rape ':\n",
    "        [\n",
    "            'raped'\n",
    "        ],\n",
    "\n",
    "    ' dumb ass':\n",
    "        [\n",
    "            'dumbass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' ass head':\n",
    "        [\n",
    "            'butthead'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            'sexy', 's3x', 'sexuality'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' mother fucker':\n",
    "        [\n",
    "             ' motha f', ' mother f', 'motherucker',\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e'\n",
    "        ],\n",
    "    \n",
    "    ' haha ':\n",
    "        [\n",
    "            'ha\\*\\*\\*ha',\n",
    "        ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternTokenizer(BaseTokenizer):\n",
    "    def __init__(self, lower=True, initial_filters=r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.' ]\", patterns=RE_PATTERNS,\n",
    "                 remove_repetitions=True):\n",
    "        self.lower = lower\n",
    "        self.patterns = patterns\n",
    "        self.initial_filters = initial_filters\n",
    "        self.remove_repetitions = remove_repetitions\n",
    "\n",
    "    def process_text(self, text):\n",
    "        x = self._preprocess(text)\n",
    "        for target, patterns in self.patterns.items():\n",
    "            for pat in patterns:\n",
    "                x = re.sub(pat, target, x)\n",
    "        x = re.sub(r\"[^a-z' ]\", ' ', x)\n",
    "        return x.split()\n",
    "\n",
    "    def process_ds(self, ds):\n",
    "        ### ds = Data series\n",
    "\n",
    "        # lower\n",
    "        ds = copy.deepcopy(ds)\n",
    "        if self.lower:\n",
    "            ds = ds.str.lower()\n",
    "            \n",
    "        # remove special chars\n",
    "        if self.initial_filters is not None:\n",
    "            ds = ds.str.replace(self.initial_filters, ' ')\n",
    "            \n",
    "        # looooooooooser = loser\n",
    "        if self.remove_repetitions:\n",
    "            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "            ds = ds.str.replace(pattern, r\"\\1\")\n",
    "\n",
    "        for target, patterns in self.patterns.items():\n",
    "            for pat in patterns:\n",
    "                ds = ds.str.replace(pat, target)\n",
    "\n",
    "        ds = ds.str.replace(r\"[^a-z' ]\", ' ')\n",
    "\n",
    "        return ds.str.split()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        # lower\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "\n",
    "        # remove special chars\n",
    "        if self.initial_filters is not None:\n",
    "            text = re.sub(self.initial_filters, ' ', text)\n",
    "\n",
    "        # fuuuuck => fuck\n",
    "        if self.remove_repetitions:\n",
    "            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "            text = pattern.sub(r\"\\1\", text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PatternTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.process_text('test your frases in here maybe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrected_text = tokenizer.process_ds(train[\"comment_text\"]).str.join(sep=\" \")\n",
    "\n",
    "# train.insert(loc=1, column='corrected_text', value=corrected_text)\n",
    "# test[\"corrected_text\"] = tokenizer.process_ds(test[\"comment_text\"]).str.join(sep=\" \")\n",
    "# train.head(100)\n",
    "\n",
    "train[\"comment_text\"] = tokenizer.process_ds(train[\"comment_text\"]).str.join(sep=\" \")\n",
    "test[\"comment_text\"] = tokenizer.process_ds(test[\"comment_text\"]).str.join(sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"data/train_preprocessed.csv\", index=False)\n",
    "test.to_csv(\"data/test_preprocessed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
